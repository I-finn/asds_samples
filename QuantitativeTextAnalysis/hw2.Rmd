---
title: 'POP77022: Programming Exercise 2'
author: "Imelda Finn"
date: "28/2/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The second homework assignment covers concepts and methods from Weeks 3 and 4 (Supervised and unsupervised text classification).  

Please provide your answers as code and text in the RMarkdown file provided. When completed, first knit the file as an HTML file and then save the resulting HTML document in PDF format.  Upload the PDF to Turnitin.

## Supervised text classification of Yelp reviews (50 points)

We begin by analyzing a sample from the Zhang, Zhao & LeCun (2015) dataset of Yelp reviews which have been coded for sentiment polarity.  The authors of the dataset have created a `sentiment` variable where a value of 1 indicates a "negative" review (1 or 2 stars), and a 2 means a "positive" review (3 or 4 stars).

First, bring in the reviews dataset from the `data` directory.  

```{r}
setwd(getwd())
data <- read.csv("./data/yelp_data_small.csv", 
                 stringsAsFactors=FALSE,
                 encoding = "utf-8")
```

1.  Create a `quanteda` corpus object from this matrix and inspect its attributes.  
    + What is the overall probability of the "positive" class in the corpus?  Are the classes balanced? (Hint: Use the `table()` function)

```{r}

```

2.  Create a document-feature matrix using this corpus.  Process the text so as to increase predictive power of the features. Justify each of your processing decisions in the context of the supervised classification task.

```{r}

```

3.  Now that you have your document-feature matrix, use the `caret` library to create a training set and testing set following an 80/20 split.

```{r}

```

4.  Using these datasets, train a naive Bayes classifier with the `caret` library to predict review sentiment.  Explain each step you take in the learning pipeline. Be sure to:
    + Evaluate the performance of the model in terms of classification accuracy of predictions in the testing set. Include a discussion of precision, recall and F1.
    + Explain in detail what steps were taken to help avoid overfitting.
    + Describe your parameter tuning.
    + Discuss the most predictive features of the dataset. (*Hint: use `kwic` to provide a qualitative context)

```{r}

```

5. Provide a similar analysis using a Support Vector Machine.  However, irrespective of your settings for Question 4, for this excercise use a 5-fold cross-validation when training the model.  Be sure to explain all steps involved as well as an evaluation of model performance.  Which model is better, NB or SVM?  Explain in detail.

```{r}

```

## Topic Modeling Breitbart News (50 points)

In this section, we will analyze the thematic structure of a corpus of news articles from Breitbart News, a right-wing American news outlet. Employ a Structural Topic Model from the `stm` library to investigate the themes found within this corpus.

First, bring in a sample of Breitbart articles from 2016 (n=5000):

```{r}
setwd(getwd())
data <- read.csv("./data/breitbart_2016_sample.csv", 
                 stringsAsFactors=FALSE,
                 encoding = "utf-8")
```

1. Process the text and generate a document-feature matrix.  Be sure to remove unhelpful characters and tokens from the DFM and to also retain the original text for model validation.  Remove tokens that occur in less than 20 documents.  Justify your feature selection decisions.

```{r}

```

2.  Convert the DFM into STM format and fit an STM model with `k=35` topics.  

```{r}

```

3.  Interpret the topics generated by the STM model.  Discuss the prevalence and top terms of each topic.  Provide a list of the labels you have associated with each estimated topic.  For each topic, justify your labelling decision. (Hint: You will want to cite excerpts from typical tweets of a given topic.  Also, use the date variable to inform estimates of topic prevalence.).  

```{r}

```

4.  Topic model validation.  Demonstrate and interpret the semantic and predictive validity of the model.  Also discuss the quality of topics in terms of semantic coherence and top exclusivity.  Discuss how you would show construct validity.

```{r}

```

5.  What insights can be gleaned about right-wing media coverage of the 2016 US election?  What election-related topics were derived from the model?  What interesting temporal patterns exist?  Why might the prevalence of certain important topics vary over 2016?  Provide evidence in support of your answers.